{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data frame is: 80793\n",
      "Shape of the data frame is:(80793, 4)\n",
      "First entries of the data frame are:                        Date  \\\n",
      "0  2022-09-29 23:41:16+00:00   \n",
      "1  2022-09-29 23:24:43+00:00   \n",
      "2  2022-09-29 23:18:08+00:00   \n",
      "3  2022-09-29 22:40:07+00:00   \n",
      "4  2022-09-29 22:27:05+00:00   \n",
      "\n",
      "                                               Tweet Stock Name Company Name  \n",
      "0  Mainstream media has done an amazing job at br...       TSLA  Tesla, Inc.  \n",
      "1  Tesla delivery estimates are at around 364k fr...       TSLA  Tesla, Inc.  \n",
      "2  3/ Even if I include 63.0M unvested RSUs as of...       TSLA  Tesla, Inc.  \n",
      "3  @RealDanODowd @WholeMarsBlog @Tesla Hahaha why...       TSLA  Tesla, Inc.  \n",
      "4  @RealDanODowd @Tesla Stop trying to kill kids,...       TSLA  Tesla, Inc.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_tweets = pd.read_csv('data/stock_tweets_80k.csv') # dataset to train the SVM on\n",
    "print(f'Length of the data frame is: {len(all_tweets)}')\n",
    "print(f'Shape of the data frame is:{all_tweets.shape}')\n",
    "print(f'First entries of the data frame are:{all_tweets.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nall_tweets = all_tweets.drop(columns=[\\'Company Name\\']) # no need for company name, remove it\\nall_tweets[\"sentiment_score\"] = \\'\\' # add data frame for sentiment score\\n\\n# make data frames for all stocks we\\'re looking at:\\n\\nstock_names = [\\'TSLA\\', \\'AMZN\\', \\'MSFT\\', \\'TSM\\']\\n\\nfor stock in stock_names:\\n    stock_df = all_tweets[all_tweets[\\'Stock Name\\'] == stock]\\n    stock_df.to_csv(f\\'filtered-stock-dataframes/{stock}-filtered-{len(stock_df)}.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1::\n",
    "\"\"\"\n",
    "all_tweets = all_tweets.drop(columns=['Company Name']) # no need for company name, remove it\n",
    "all_tweets[\"sentiment_score\"] = '' # add data frame for sentiment score\n",
    "\n",
    "# make data frames for all stocks we're looking at:\n",
    "\n",
    "stock_names = ['TSLA', 'AMZN', 'MSFT', 'TSM']\n",
    "\n",
    "for stock in stock_names:\n",
    "    stock_df = all_tweets[all_tweets['Stock Name'] == stock]\n",
    "    stock_df.to_csv(f'filtered-stock-dataframes/{stock}-filtered-{len(stock_df)}.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tarkojuss/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tarkojuss/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/tarkojuss/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First entries of the data frame with sentiment added:                         Date  \\\n",
      "0  2022-09-29 23:41:16+00:00   \n",
      "1  2022-09-29 23:24:43+00:00   \n",
      "2  2022-09-29 23:18:08+00:00   \n",
      "3  2022-09-29 22:40:07+00:00   \n",
      "4  2022-09-29 22:27:05+00:00   \n",
      "\n",
      "                                               Tweet Stock Name Company Name  \\\n",
      "0  Mainstream medium done amazing job brainwashin...       TSLA  Tesla, Inc.   \n",
      "1  Tesla delivery estimate around 364k analysts. ...       TSLA  Tesla, Inc.   \n",
      "2  3/ Even I include 63.0M unvested RSUs 6/30, ad...       TSLA  Tesla, Inc.   \n",
      "3  Hahaha still trying stop Tesla FSD bro! Get sh...       TSLA  Tesla, Inc.   \n",
      "4        Stop trying kill kids, sad deranged old man       TSLA  Tesla, Inc.   \n",
      "\n",
      "   sentiment_score  \n",
      "0           0.0772  \n",
      "1           0.0000  \n",
      "2           0.2960  \n",
      "3          -0.4559  \n",
      "4          -0.8750  \n"
     ]
    }
   ],
   "source": [
    "# 1.2::\n",
    "from textblob import Word\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_tweets(tweet):\n",
    "    processed_tweet = tweet.lower()\n",
    "    processed_tweet = re.sub(r'http\\S+', '', processed_tweet)\n",
    "    processed_tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    processed_tweet.replace('[^\\w\\s]', '')\n",
    "    processed_tweet = \" \".join(word for word in processed_tweet.split() if word not in stopwords.words('english'))\n",
    "    processed_tweet = \" \".join(Word(word).lemmatize() for word in processed_tweet.split())\n",
    "    return processed_tweet\n",
    "\n",
    "all_tweets['Tweet'] = all_tweets['Tweet'].apply(preprocess_tweets)\n",
    "\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "for index, row in all_tweets.T.iteritems():\n",
    "    try:\n",
    "        sentence_i = unicodedata.normalize('NFKD', all_tweets.loc[index, 'Tweet'])\n",
    "        sentence_sentiment = sentiment_analyzer.polarity_scores(sentence_i)\n",
    "        all_tweets.at[index, 'sentiment_score'] = sentence_sentiment['compound']\n",
    "    except TypeError as e:\n",
    "        print(f'error: {e}')\n",
    "        break\n",
    "\n",
    "print(f'First entries of the data frame with sentiment added: {all_tweets.head()}')\n",
    "\n",
    "# convert to binary sentiments:\n",
    "\n",
    "def assign_binary_sentiment(x):\n",
    "    if x > 0.05: return 1\n",
    "    elif x < 0.05: return 0\n",
    "    else: return None\n",
    "    \n",
    "all_tweets['binary_sentiment'] = all_tweets['sentiment_score'].apply(assign_binary_sentiment)\n",
    "all_tweets.to_csv(f'all_tweets_sentiment.csv', index=False) # save edited dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3::\n",
    "\n",
    "# full data frame split:\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_data = pd.read_csv('all_tweets_sentiment.csv')\n",
    "\n",
    "svm_data.dropna(subset=['binary_sentiment'], inplace=True)\n",
    "\n",
    "y = svm_data['binary_sentiment'].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(svm_data['Tweet'].values, y, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=1, \n",
    "                                                    test_size=0.3, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# vectorize the full data frame:\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english') # vectorize the data rame\n",
    "\n",
    "x_train = np.where(pd.isnull(x_train), '', x_train)\n",
    "x_test = np.where(pd.isnull(x_test), '', x_test)\n",
    "\n",
    "vectorizer.fit(list(x_train) + list(x_test)) # learn a vocab\n",
    "\n",
    "x_train_vec = vectorizer.transform(x_train) # transform documents to document-term matrix\n",
    "x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "# pd.DataFrame(x_train_vec.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy score for the SVC is:  92.47050086640812 %\n"
     ]
    }
   ],
   "source": [
    "# 1.3::\n",
    "\n",
    "# train the SVM classifier:\n",
    "\n",
    "svm = svm.SVC(kernel = 'linear', C = 1)\n",
    "prob = svm.fit(x_train_vec, y_train)\n",
    "y_pred_svm = svm.predict(x_test_vec)\n",
    "\n",
    "print(\"Overall accuracy score for the SVC is: \", accuracy_score(y_test, y_pred_svm) * 100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11226   838]\n",
      " [  987 11187]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92     12064\n",
      "           1       0.93      0.92      0.92     12174\n",
      "\n",
      "    accuracy                           0.92     24238\n",
      "   macro avg       0.92      0.92      0.92     24238\n",
      "weighted avg       0.92      0.92      0.92     24238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.4::\n",
    "predictions = []\n",
    "svm_data['Tweet'] = np.where(pd.isnull(svm_data['Tweet']), '', svm_data['Tweet'])\n",
    "for tweet in svm_data['Tweet']:\n",
    "    tweet_vec = vectorizer.transform([tweet])\n",
    "    prediction = svm.predict(tweet_vec)[0]\n",
    "    predictions.append(prediction)\n",
    "\n",
    "svm_data['prediction'] = predictions\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "report = classification_report(y_test, y_pred_svm)\n",
    "confusion = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "print(confusion)\n",
    "print(report)\n",
    "\n",
    "svm_data.to_csv(f'final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions of 0: 40372\n",
      "Number of predictions of 1: 40421\n",
      "Portion of positive over negative sentiments: 0.5003032440929288\n"
     ]
    }
   ],
   "source": [
    "final = pd.read_csv('final.csv')\n",
    "prediction_counts = final['prediction'].value_counts()\n",
    "\n",
    "print(\"Number of predictions of 0:\", prediction_counts[0])\n",
    "print(\"Number of predictions of 1:\", prediction_counts[1])\n",
    "print(f'Portion of positive over negative sentiments: {prediction_counts[1] / (prediction_counts[0] + prediction_counts[1])}')\n",
    "\n",
    "predictions_1 = final[final['prediction'] == 1]['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dates = final['Date'].str[:10].unique()\n",
    "dates_df = pd.DataFrame({'Date': unique_dates})\n",
    "\n",
    "positive_counts = []\n",
    "negative_counts = []\n",
    "positive_percentages = []\n",
    "\n",
    "for date in unique_dates:\n",
    "    \n",
    "    positive_count = len(final[(final['Date'].str[:10] == date) & (final['prediction'] == 1)])\n",
    "    negative_count = len(final[(final['Date'].str[:10] == date) & (final['prediction'] == 0)])\n",
    "    positive_counts.append(positive_count)\n",
    "    negative_counts.append(negative_count)\n",
    "\n",
    "    total_count = positive_count + negative_count\n",
    "    positive_percentage = (positive_count / total_count) * 100 if total_count != 0 else 0\n",
    "\n",
    "    positive_percentages.append(positive_percentage)\n",
    "\n",
    "dates_df['positive_count'] = positive_counts\n",
    "dates_df['negative_count'] = negative_counts\n",
    "dates_df['positive_percentage'] = positive_percentages\n",
    "\n",
    "dates_df.to_csv('dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def final_analysis(stock):\n",
    "\n",
    "    stonks = pd.read_csv(f'stock-data/{stock}-historical-data.csv')\n",
    "    stonks['DATE'] = stonks['DATE'].str[:10]\n",
    "\n",
    "    merged_df = stonks.merge(dates_df, left_on='DATE', right_on='Date', how='left')\n",
    "    stonks['positive_percentage'] = merged_df['positive_percentage']\n",
    "\n",
    "    merged_df.to_csv('merged.csv', index=False)\n",
    "    print(f'Historical data has been merged with positive sentiment for ${stock}')\n",
    "\n",
    "    # monthly averages for positive percentages:\n",
    "\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "    merged_df['month'] = merged_df['Date'].dt.to_period('M')\n",
    "    monthly_avg = merged_df.groupby('month')['positive_percentage'].mean()\n",
    "    monthly_avg_df = pd.DataFrame({'month': monthly_avg.index, 'mean_pos_percentage': monthly_avg.values})\n",
    "    monthly_avg_df.set_index('month', inplace=True)\n",
    "\n",
    "    monthly_avg_df.to_csv('monthly_avgs.csv', index=True)\n",
    "\n",
    "    # weekly averages for positive percentages:\n",
    "\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "    merged_df['week'] = merged_df['Date'].dt.to_period('W')\n",
    "    weekly_avg = merged_df.groupby('week')['positive_percentage'].mean()\n",
    "    weekly_avg_df = pd.DataFrame({'week': weekly_avg.index, 'mean_pos_percentage': weekly_avg.values})\n",
    "    weekly_avg_df.set_index('week', inplace=True)\n",
    "\n",
    "    weekly_avg_df.to_csv('weekly_avgs.csv', index=True)\n",
    "\n",
    "    # yearly averages for positive percentages:\n",
    "\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "    merged_df['year'] = merged_df['Date'].dt.to_period('Y')\n",
    "    yearly_avg = merged_df.groupby('year')['positive_percentage'].mean()\n",
    "    yearly_avg_df = pd.DataFrame({'year': yearly_avg.index, 'mean_pos_percentage': yearly_avg.values})\n",
    "    yearly_avg_df.set_index('year', inplace=True)\n",
    "\n",
    "    yearly_avg_df.to_csv('yearly_avgs.csv', index=True)\n",
    "\n",
    "    # how many are correct on the daily timeframe:\n",
    "\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "    merged_df['day'] = merged_df['Date'].dt.to_period('D')\n",
    "    daily_avg = merged_df.groupby('day')['positive_percentage'].mean()\n",
    "    daily_avg_df = pd.DataFrame({'day': daily_avg.index, 'mean_pos_percentage': daily_avg.values})\n",
    "    daily_avg_df.set_index('day', inplace=True)\n",
    "\n",
    "    daily_avg_df.to_csv('daily_avgs.csv', index=True)\n",
    "\n",
    "    merged_df['final_decision'] = float('nan') # add another row for final decisions\n",
    "\n",
    "    # get final decisions:\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        if row['% CHANGE'] >= 0 and row['positive_percentage'] > 50:\n",
    "            merged_df.loc[index, 'final_decision'] = 1\n",
    "        elif row['% CHANGE'] < 0 and row['positive_percentage'] < 50:\n",
    "            merged_df.loc[index, 'final_decision'] = 1\n",
    "        else:\n",
    "            merged_df.loc[index, 'final_decision'] = 0\n",
    "\n",
    "    merged_df.to_csv(f'final_values_{stock}.csv', index=True)# assessor, please keep in mind that here we save a .csv with a lot of empty values, but they are removed after reading them back in\n",
    "\n",
    "    final_df = pd.read_csv(f'final_values_{stock}.csv') # assessor, please keep in mind that here we save a \n",
    "    final_df.dropna(subset=['Date'], inplace=True) # drop all empty values to get meaningful counts\n",
    "    counts = final_df['final_decision'].value_counts()\n",
    "    print(counts)\n",
    "\n",
    "    print(f'\\n---\\nOut of {counts[0] + counts[1]} final decisions for ${stock}, {round( ( counts[1] / (counts[0] + counts[1])) * 100, 2)}% were correct.\\n---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical data has been merged with positive sentiment for $TSLA\n",
      "0.0    133\n",
      "1.0    119\n",
      "Name: final_decision, dtype: int64\n",
      "\n",
      "---\n",
      "Out of 252 final decisions for $TSLA, 47.22% were correct.\n",
      "---\n",
      "Historical data has been merged with positive sentiment for $TSM\n",
      "0.0    132\n",
      "1.0    120\n",
      "Name: final_decision, dtype: int64\n",
      "\n",
      "---\n",
      "Out of 252 final decisions for $TSM, 47.62% were correct.\n",
      "---\n",
      "Historical data has been merged with positive sentiment for $AAPL\n",
      "0.0    133\n",
      "1.0    119\n",
      "Name: final_decision, dtype: int64\n",
      "\n",
      "---\n",
      "Out of 252 final decisions for $AAPL, 47.22% were correct.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for stock in ['TSLA', 'TSM', 'AAPL']:\n",
    "    final_analysis(stock)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    week  mean_pos_percentage\n",
      "0  2021-09-27/2021-10-03            59.074037\n",
      "1  2021-10-04/2021-10-10            50.168386\n",
      "2  2021-10-11/2021-10-17            51.052187\n",
      "3  2021-10-18/2021-10-24            51.082044\n",
      "4  2021-10-25/2021-10-31            46.537857\n",
      "5  2021-11-01/2021-11-07            49.926031\n",
      "6  2021-11-08/2021-11-14            53.840016\n",
      "7  2021-11-15/2021-11-21            52.738986\n",
      "8  2021-11-22/2021-11-28            51.172422\n",
      "9  2021-11-29/2021-12-05            51.269295\n",
      "     month  mean_pos_percentage\n",
      "0  2021-09            60.112360\n",
      "1  2021-10            50.106575\n",
      "2  2021-11            52.133225\n",
      "3  2021-12            51.568384\n",
      "4  2022-01            50.412504\n",
      "5  2022-02            50.242831\n",
      "6  2022-03            50.536536\n",
      "7  2022-04            46.251405\n",
      "8  2022-05            47.290097\n",
      "9  2022-06            47.989704\n",
      "   year  mean_pos_percentage\n",
      "0  2021            51.410040\n",
      "1  2022            49.251341\n"
     ]
    }
   ],
   "source": [
    "# visualize the time frames for Appendix A:\n",
    "\n",
    "for df in ['weekly', 'monthly', 'yearly']:\n",
    "    df = pd.read_csv(f'{df}_avgs.csv')\n",
    "    print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    29\n",
      "0    24\n",
      "Name: decision, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "compare = pd.read_csv(f'weekly_avgs.csv')\n",
    "weeklychange = pd.read_csv('stock-data-timeframes/TSLA-weekly_change.csv')\n",
    "\n",
    "compare['DATE'] = compare['DATE'].apply(lambda x: x[-10:])\n",
    "compare['DATE'] = compare['DATE'].astype(str)\n",
    "weeklychange['DATE'] = weeklychange['DATE'].astype(str)\n",
    "weeklychange['decision'] = 0\n",
    "\n",
    "merged_df = pd.merge(compare, weeklychange, on='DATE')\n",
    "\n",
    "merged_df.loc[(merged_df['TICKER'] > 0) & (merged_df['mean_pos_percentage'] > 50), 'decision'] = 1\n",
    "merged_df.loc[(merged_df['TICKER'] < 0) & (merged_df['mean_pos_percentage'] < 50), 'decision'] = 1\n",
    "\n",
    "counts = merged_df['decision'].value_counts()\n",
    "print(counts)\n",
    "\n",
    "merged_df.to_csv(f'decisions_weekly.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    9\n",
      "0    3\n",
      "Name: decision, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "compare = pd.read_csv(f'monthly_avgs.csv')\n",
    "monthlychange = pd.read_csv('stock-data-timeframes/TSLA-monthly_change.csv')\n",
    "\n",
    "compare['DATE'] = compare['DATE'].apply(lambda x: x[-10:])\n",
    "compare['DATE'] = compare['DATE'].astype(str)\n",
    "monthlychange['DATE'] = monthlychange['DATE'].astype(str)\n",
    "monthlychange['decision'] = 0\n",
    "\n",
    "merged_df = pd.merge(compare, monthlychange, on='DATE')\n",
    "\n",
    "merged_df.loc[(merged_df['CLOSING_PRICE'] > 0) & (merged_df['mean_pos_percentage'] > 50), 'decision'] = 1\n",
    "merged_df.loc[(merged_df['CLOSING_PRICE'] < 0) & (merged_df['mean_pos_percentage'] < 50), 'decision'] = 1\n",
    "\n",
    "counts = merged_df['decision'].value_counts()\n",
    "print(counts)\n",
    "\n",
    "merged_df.to_csv(f'decisions_monthly.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
