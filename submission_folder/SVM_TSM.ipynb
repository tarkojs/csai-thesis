{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data frame is: 11034\n",
      "Shape of the data frame is:(11034, 4)\n",
      "First entries of the data frame are:                        Date  \\\n",
      "0  2022-09-29 22:23:02+00:00   \n",
      "1  2022-09-29 21:50:16+00:00   \n",
      "2  2022-09-29 21:13:20+00:00   \n",
      "3  2022-09-29 19:35:26+00:00   \n",
      "4  2022-09-29 19:19:05+00:00   \n",
      "\n",
      "                                               Tweet Stock Name  \\\n",
      "0  EG vs FNC is a deadly matchup.\\n\\nEG using the...        TSM   \n",
      "1  @BTSsmash @coinbase @Panda_Plup @LiquidHbox @T...        TSM   \n",
      "2  I keep looking at the current #R6NAL standings...        TSM   \n",
      "3  @BTSsmash @coinbase @Panda_Plup @LiquidHbox @T...        TSM   \n",
      "4  @joe1chief @BTSsmash @coinbase @Panda_Plup @Li...        TSM   \n",
      "\n",
      "                                        Company Name  \n",
      "0  Taiwan Semiconductor Manufacturing Company Lim...  \n",
      "1  Taiwan Semiconductor Manufacturing Company Lim...  \n",
      "2  Taiwan Semiconductor Manufacturing Company Lim...  \n",
      "3  Taiwan Semiconductor Manufacturing Company Lim...  \n",
      "4  Taiwan Semiconductor Manufacturing Company Lim...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_tweets = pd.read_csv('TSM-dataset-11034.csv') # dataset to train the SVM on\n",
    "print(f'Length of the data frame is: {len(all_tweets)}')\n",
    "print(f'Shape of the data frame is:{all_tweets.shape}')\n",
    "print(f'First entries of the data frame are:{all_tweets.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nall_tweets = all_tweets.drop(columns=[\\'Company Name\\']) # no need for company name, remove it\\nall_tweets[\"sentiment_score\"] = \\'\\' # add data frame for sentiment score\\n\\n# make data frames for all stocks we\\'re looking at:\\n\\nstock_names = [\\'TSLA\\', \\'AMZN\\', \\'MSFT\\', \\'TSM\\']\\n\\nfor stock in stock_names:\\n    stock_df = all_tweets[all_tweets[\\'Stock Name\\'] == stock]\\n    stock_df.to_csv(f\\'filtered-stock-dataframes/{stock}-filtered-{len(stock_df)}.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1::\n",
    "\"\"\"\n",
    "all_tweets = all_tweets.drop(columns=['Company Name']) # no need for company name, remove it\n",
    "all_tweets[\"sentiment_score\"] = '' # add data frame for sentiment score\n",
    "\n",
    "# make data frames for all stocks we're looking at:\n",
    "\n",
    "stock_names = ['TSLA', 'AMZN', 'MSFT', 'TSM']\n",
    "\n",
    "for stock in stock_names:\n",
    "    stock_df = all_tweets[all_tweets['Stock Name'] == stock]\n",
    "    stock_df.to_csv(f'filtered-stock-dataframes/{stock}-filtered-{len(stock_df)}.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tarkojuss/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tarkojuss/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/tarkojuss/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First entries of the data frame with sentiment added:                         Date  \\\n",
      "0  2022-09-29 22:23:02+00:00   \n",
      "1  2022-09-29 21:50:16+00:00   \n",
      "2  2022-09-29 21:13:20+00:00   \n",
      "3  2022-09-29 19:35:26+00:00   \n",
      "4  2022-09-29 19:19:05+00:00   \n",
      "\n",
      "                                               Tweet Stock Name  \\\n",
      "0  EG v FNC deadly matchup. EG using strat v TSM ...        TSM   \n",
      "1                                    ken seething rn        TSM   \n",
      "2  I keep looking current #R6NAL standing I still...        TSM   \n",
      "3                                               cool        TSM   \n",
      "4                          come last summit covid :(        TSM   \n",
      "\n",
      "                                        Company Name  sentiment_score  \n",
      "0  Taiwan Semiconductor Manufacturing Company Lim...          -0.4215  \n",
      "1  Taiwan Semiconductor Manufacturing Company Lim...           0.0000  \n",
      "2  Taiwan Semiconductor Manufacturing Company Lim...           0.4404  \n",
      "3  Taiwan Semiconductor Manufacturing Company Lim...           0.3182  \n",
      "4  Taiwan Semiconductor Manufacturing Company Lim...          -0.4404  \n"
     ]
    }
   ],
   "source": [
    "# 1.2::\n",
    "from textblob import Word\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_tweets(tweet):\n",
    "    processed_tweet = tweet.lower()\n",
    "    processed_tweet = re.sub(r'http\\S+', '', processed_tweet)\n",
    "    processed_tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    processed_tweet.replace('[^\\w\\s]', '')\n",
    "    processed_tweet = \" \".join(word for word in processed_tweet.split() if word not in stopwords.words('english'))\n",
    "    processed_tweet = \" \".join(Word(word).lemmatize() for word in processed_tweet.split())\n",
    "    return processed_tweet\n",
    "\n",
    "all_tweets['Tweet'] = all_tweets['Tweet'].apply(preprocess_tweets)\n",
    "\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "for index, row in all_tweets.T.iteritems():\n",
    "    try:\n",
    "        sentence_i = unicodedata.normalize('NFKD', all_tweets.loc[index, 'Tweet'])\n",
    "        sentence_sentiment = sentiment_analyzer.polarity_scores(sentence_i)\n",
    "        all_tweets.at[index, 'sentiment_score'] = sentence_sentiment['compound']\n",
    "    except TypeError as e:\n",
    "        print(f'error: {e}')\n",
    "        break\n",
    "\n",
    "print(f'First entries of the data frame with sentiment added: {all_tweets.head()}')\n",
    "\n",
    "# convert to binary sentiments:\n",
    "\n",
    "def assign_binary_sentiment(x):\n",
    "    if x > 0.05: return 1\n",
    "    elif x < 0.05: return 0\n",
    "    else: return None # return None if sentiment is neutral\n",
    "    \n",
    "all_tweets['binary_sentiment'] = all_tweets['sentiment_score'].apply(assign_binary_sentiment)\n",
    "all_tweets.to_csv(f'all_tweets_sentiment.csv', index=False) # save edited dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3::\n",
    "\n",
    "# full data frame split:\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_data = pd.read_csv('all_tweets_sentiment.csv')\n",
    "\n",
    "svm_data.dropna(subset=['binary_sentiment'], inplace=True)\n",
    "\n",
    "y = svm_data['binary_sentiment'].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(svm_data['Tweet'].values, y, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=1, \n",
    "                                                    test_size=0.3, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# vectorize the full data frame:\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english') # vectorize the data rame\n",
    "\n",
    "x_train = np.where(pd.isnull(x_train), '', x_train)\n",
    "x_test = np.where(pd.isnull(x_test), '', x_test)\n",
    "\n",
    "vectorizer.fit(list(x_train) + list(x_test)) # learn a vocab\n",
    "\n",
    "x_train_vec = vectorizer.transform(x_train) # transform documents to document-term matrix\n",
    "x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "# pd.DataFrame(x_train_vec.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy score for the SVC is:  87.19420114768953 %\n"
     ]
    }
   ],
   "source": [
    "# 1.3::\n",
    "\n",
    "# train the SVM classifier:\n",
    "\n",
    "svm = svm.SVC(kernel = 'linear', C = 1)\n",
    "prob = svm.fit(x_train_vec, y_train)\n",
    "y_pred_svm = svm.predict(x_test_vec)\n",
    "\n",
    "print(\"Overall accuracy score for the SVC is: \", accuracy_score(y_test, y_pred_svm) * 100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1464  167]\n",
      " [ 257 1423]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87      1631\n",
      "           1       0.89      0.85      0.87      1680\n",
      "\n",
      "    accuracy                           0.87      3311\n",
      "   macro avg       0.87      0.87      0.87      3311\n",
      "weighted avg       0.87      0.87      0.87      3311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# append the SVM predictions to the data frame:\n",
    "\n",
    "predictions = []\n",
    "svm_data['Tweet'] = np.where(pd.isnull(svm_data['Tweet']), '', svm_data['Tweet'])\n",
    "for tweet in svm_data['Tweet']:\n",
    "    tweet_vec = vectorizer.transform([tweet])\n",
    "    prediction = svm.predict(tweet_vec)[0]\n",
    "    predictions.append(prediction)\n",
    "\n",
    "svm_data['prediction'] = predictions\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "report = classification_report(y_test, y_pred_svm)\n",
    "confusion = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "print(confusion)\n",
    "print(report)\n",
    "\n",
    "svm_data.to_csv(f'final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions of 0: 5537\n",
      "Number of predictions of 1: 5497\n",
      "Portion of positive over negative sentiments: 0.4981874206996556\n"
     ]
    }
   ],
   "source": [
    "final = pd.read_csv('final.csv')\n",
    "prediction_counts = final['prediction'].value_counts()\n",
    "\n",
    "print(\"Number of predictions of 0:\", prediction_counts[0])\n",
    "print(\"Number of predictions of 1:\", prediction_counts[1])\n",
    "print(f'Portion of positive over negative sentiments: {prediction_counts[1] / (prediction_counts[0] + prediction_counts[1])}')\n",
    "\n",
    "predictions_1 = final[final['prediction'] == 1]['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dates = final['Date'].str[:10].unique()\n",
    "dates_df = pd.DataFrame({'Date': unique_dates})\n",
    "\n",
    "positive_counts = []\n",
    "negative_counts = []\n",
    "positive_percentages = []\n",
    "\n",
    "for date in unique_dates:\n",
    "    \n",
    "    positive_count = len(final[(final['Date'].str[:10] == date) & (final['prediction'] == 1)])\n",
    "    negative_count = len(final[(final['Date'].str[:10] == date) & (final['prediction'] == 0)])\n",
    "    positive_counts.append(positive_count)\n",
    "    negative_counts.append(negative_count)\n",
    "\n",
    "    total_count = positive_count + negative_count\n",
    "    positive_percentage = (positive_count / total_count) * 100 if total_count != 0 else 0\n",
    "\n",
    "    positive_percentages.append(positive_percentage)\n",
    "\n",
    "dates_df['positive_count'] = positive_counts\n",
    "dates_df['negative_count'] = negative_counts\n",
    "dates_df['positive_percentage'] = positive_percentages\n",
    "\n",
    "dates_df.to_csv('dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def final_analysis(stock):\n",
    "\n",
    "    stonks = pd.read_csv(f'stock-data/{stock}-historical-data.csv')\n",
    "    stonks['DATE'] = stonks['DATE'].str[:10]\n",
    "\n",
    "    merged_df = stonks.merge(dates_df, left_on='DATE', right_on='Date', how='left')\n",
    "    stonks['positive_percentage'] = merged_df['positive_percentage']\n",
    "\n",
    "    merged_df.to_csv('merged.csv', index=False)\n",
    "    print(f'Historical data has been merged with positive sentiment for ${stock}')\n",
    "\n",
    "    # monthly averages for positive percentages:\n",
    "\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "    merged_df['month'] = merged_df['Date'].dt.to_period('M')\n",
    "    monthly_avg = merged_df.groupby('month')['positive_percentage'].mean()\n",
    "    monthly_avg_df = pd.DataFrame({'month': monthly_avg.index, 'mean_pos_percentage': monthly_avg.values})\n",
    "    monthly_avg_df.set_index('month', inplace=True)\n",
    "\n",
    "    monthly_avg_df.to_csv('monthly_avgs.csv', index=True)\n",
    "\n",
    "    # weekly averages for positive percentages:\n",
    "\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "    merged_df['week'] = merged_df['Date'].dt.to_period('W')\n",
    "    weekly_avg = merged_df.groupby('week')['positive_percentage'].mean()\n",
    "    weekly_avg_df = pd.DataFrame({'week': weekly_avg.index, 'mean_pos_percentage': weekly_avg.values})\n",
    "    weekly_avg_df.set_index('week', inplace=True)\n",
    "\n",
    "    weekly_avg_df.to_csv('weekly_avgs.csv', index=True)\n",
    "\n",
    "    # yearly averages for positive percentages:\n",
    "\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "    merged_df['year'] = merged_df['Date'].dt.to_period('Y')\n",
    "    yearly_avg = merged_df.groupby('year')['positive_percentage'].mean()\n",
    "    yearly_avg_df = pd.DataFrame({'year': yearly_avg.index, 'mean_pos_percentage': yearly_avg.values})\n",
    "    yearly_avg_df.set_index('year', inplace=True)\n",
    "\n",
    "    yearly_avg_df.to_csv('yearly_avgs.csv', index=True)\n",
    "\n",
    "    # how many are correct on the daily timeframe:\n",
    "\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "    merged_df['day'] = merged_df['Date'].dt.to_period('D')\n",
    "    daily_avg = merged_df.groupby('day')['positive_percentage'].mean()\n",
    "    daily_avg_df = pd.DataFrame({'day': daily_avg.index, 'mean_pos_percentage': daily_avg.values})\n",
    "    daily_avg_df.set_index('day', inplace=True)\n",
    "\n",
    "    daily_avg_df.to_csv('daily_avgs.csv', index=True)\n",
    "\n",
    "    merged_df['final_decision'] = float('nan') # add another row for final decisions\n",
    "\n",
    "    # get final decisions:\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        if row['% CHANGE'] >= 0 and row['positive_percentage'] > 50:\n",
    "            merged_df.loc[index, 'final_decision'] = 1\n",
    "        elif row['% CHANGE'] < 0 and row['positive_percentage'] < 50:\n",
    "            merged_df.loc[index, 'final_decision'] = 1\n",
    "        else:\n",
    "            merged_df.loc[index, 'final_decision'] = 0\n",
    "\n",
    "    merged_df.to_csv(f'final_values_{stock}.csv', index=True)\n",
    "    counts = merged_df['final_decision'].value_counts()\n",
    "\n",
    "    final_df = pd.read_csv(f'final_values_{stock}.csv')\n",
    "    final_df.dropna(subset=['Date'], inplace=True) # drop all empty values to get meaningful counts\n",
    "    counts = final_df['final_decision'].value_counts()\n",
    "\n",
    "    print(f'\\n---\\nOut of {counts[0] + counts[1]} final decisions for ${stock}, {round( ( counts[1] / (counts[0] + counts[1])) * 100, 2)}% were correct.\\n---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical data has been merged with positive sentiment for $TSLA\n",
      "\n",
      "---\n",
      "Out of 252 final decisions for $TSLA, 46.03% were correct.\n",
      "---\n",
      "Historical data has been merged with positive sentiment for $TSM\n",
      "\n",
      "---\n",
      "Out of 252 final decisions for $TSM, 42.46% were correct.\n",
      "---\n",
      "Historical data has been merged with positive sentiment for $AAPL\n",
      "\n",
      "---\n",
      "Out of 252 final decisions for $AAPL, 46.43% were correct.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for stock in ['TSLA', 'TSM', 'AAPL']:\n",
    "    final_analysis(stock)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
